{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4ae0bff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, root_mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3ab8de94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8990, 98)\n",
      "D\n",
      "Added 324 new D features\n",
      "E\n",
      "Added 720 new E features\n",
      "I\n",
      "Added 324 new I features\n",
      "M\n",
      "Added 648 new M features\n",
      "P\n",
      "Added 468 new P features\n",
      "S\n",
      "Added 432 new S features\n",
      "V\n",
      "Added 468 new V features\n",
      "(8990, 3617)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"kaggle/input/hull-tactical-market-prediction/train.csv\")\n",
    "EXCLUDED_COLS = {'date', 'date_id', 'forward_returns', 'risk_free_rate', 'market_forward_excess_returns', 'log_market_forward_excess_returns'}\n",
    "ARITH_TARGET_COL = \"market_forward_excess_returns\"\n",
    "TARGET_COL = \"log_market_forward_excess_returns\"\n",
    "DATE_COL = \"date_id\"\n",
    "SENTINAL_VALUE = -9999\n",
    "START_DATE = \"1990-01-01\"\n",
    "N_SAMPLES = len(df)\n",
    "\n",
    "def add_date_col(df, start_date, periods):\n",
    "    df = df.copy()\n",
    "    dates = pd.date_range(start=start_date, periods=periods, freq=\"B\")\n",
    "    df[\"date\"] = dates.values\n",
    "    return df\n",
    "\n",
    "def sort_data_by_date_id(df, date_col):\n",
    "    return df.sort_values(date_col).reset_index(drop=True)\n",
    "\n",
    "def create_target_log(df, target_col, arith_target_col):\n",
    "    df = df.copy()\n",
    "    df[target_col] = np.log1p(df[arith_target_col])\n",
    "    return df\n",
    "\n",
    "def create_category_groups(df, excluded_col):\n",
    "    category_groups = {\n",
    "        col[0]: []\n",
    "        for col in df.columns\n",
    "        if col[0].isalpha() and col not in excluded_col\n",
    "    }\n",
    "    for dfcol in df.columns:\n",
    "        if dfcol in excluded_col:\n",
    "            continue\n",
    "        if dfcol and dfcol[0].isalpha() and dfcol[0] in category_groups:\n",
    "            category = dfcol[0]\n",
    "            category_groups[category].append(dfcol)\n",
    "\n",
    "    return category_groups\n",
    "\n",
    "def construct_momentum_cols(\n",
    "        df, target_col, sentinal, category_groups,\n",
    "        windows=[5, 10, 21, 63, 126, 252],\n",
    "        lags=[1, 2, 3, 5, 10, 21, 63, 126]\n",
    "        ):\n",
    "    df_momentum = pd.DataFrame(index=df.index)\n",
    "\n",
    "    target_shifted = df[target_col].shift(1)\n",
    "\n",
    "    for window in windows:\n",
    "        df_momentum[f'MOM_mean_{window}'] = target_shifted.rolling(window).mean()\n",
    "        df_momentum[f'MOM_std_{window}'] = target_shifted.rolling(window).std()\n",
    "        df_momentum[f'MOM_roc_{window}'] = target_shifted / target_shifted.shift(window) - 1\n",
    "        df_momentum[f'MOM_cum_{window}'] = target_shifted.rolling(window).sum()\n",
    "\n",
    "    df_momentum['MOM_ema_fast'] = target_shifted.ewm(span=10, adjust=False).mean()\n",
    "    df_momentum['MOM_ema_slow'] = target_shifted.ewm(span=50, adjust=False).mean()\n",
    "    df_momentum['MOM_ema_diff'] = df_momentum['MOM_ema_fast'] - df_momentum['MOM_ema_slow']\n",
    "    df_momentum['MOM_ema_ratio'] = df_momentum['MOM_ema_fast'] / df_momentum['MOM_ema_slow'] - 1\n",
    "\n",
    "    df_momentum['MOM_std_21_252_ratio'] = df_momentum['MOM_std_21'] / df_momentum['MOM_std_252']\n",
    "\n",
    "    for lag in lags:\n",
    "        df_momentum[f'MOM_lag_{lag}'] = df[target_col].shift(lag)\n",
    "\n",
    "    df_momentum = df_momentum.fillna(sentinal)\n",
    "\n",
    "    mom_cols = df_momentum.columns\n",
    "\n",
    "    if 'MOM' not in category_groups:\n",
    "        category_groups[\"MOM\"] = []\n",
    "    category_groups[\"MOM\"].extend(mom_cols)\n",
    "\n",
    "    df_out = pd.concat([df, df_momentum], axis=1)\n",
    "\n",
    "    return df_out, category_groups\n",
    "\n",
    "def construct_volatility_indicators(\n",
    "            df, target_col, sentinal, category_groups\n",
    "        ):\n",
    "    df_volatility_indicators = pd.DataFrame(index=df.index)\n",
    "\n",
    "    target_shifted = df[target_col].shift(1)\n",
    "\n",
    "    df_volatility_indicators[\"VI_regime_highvol\"] = (df[\"MOM_std_21_252_ratio\"] > 1.2).astype(\"int8\")\n",
    "\n",
    "    z = target_shifted / (target_shifted.shift(1).rolling(21).std())\n",
    "    df_volatility_indicators[\"VI_regime_shock\"] = (z.abs() > 2.5).astype(\"int8\")\n",
    "\n",
    "    mkt = (1 + target_shifted.fillna(sentinal)).cumprod()\n",
    "    sma200 = mkt.rolling(200).mean()\n",
    "    df_volatility_indicators[\"VI_regime_bull\"] = (mkt > sma200).astype(\"int8\")\n",
    "\n",
    "    df_volatility_indicators[\"VI_volofvol_21\"] = df[\"MOM_std_21\"].pct_change().fillna(0)\n",
    "\n",
    "    for col in [\"MOM_ema_diff\", \"MOM_roc_21\", \"MOM_mean_21\"]:\n",
    "        if col in df:\n",
    "            df_volatility_indicators[f\"VI_{col}_x_bull\"] = df[col] * df_volatility_indicators[\"VI_regime_bull\"]\n",
    "            df_volatility_indicators[f\"VI_{col}_x_highvol\"] = df[col] * df_volatility_indicators[\"VI_regime_highvol\"]\n",
    "\n",
    "    df_volatility_indicators[\"VI_regime_highvol_x_ema\"] = df_volatility_indicators[\"VI_regime_highvol\"] * df.get(\"MOM_ema_diff\", 0)\n",
    "\n",
    "    vi_cols = df_volatility_indicators.columns\n",
    "\n",
    "    if 'VI' not in category_groups:\n",
    "        category_groups[\"VI\"] = []\n",
    "    category_groups[\"VI\"].extend(vi_cols)\n",
    "\n",
    "    df_out = pd.concat([df, df_volatility_indicators], axis=1)\n",
    "\n",
    "    return df_out, category_groups\n",
    "\n",
    "def impute_independent_variables(df, sentinal, excluded):\n",
    "    df = df.copy()\n",
    "    feature_cols = [col for col in df.columns if col not in excluded]\n",
    "    flag_cols = []\n",
    "\n",
    "    for col in feature_cols:\n",
    "        if df[col].isnull().any():\n",
    "            flag_col_name = f'{col}_is_missing'\n",
    "            flag_cols.append(flag_col_name)\n",
    "            df[flag_col_name] = df[col].isnull().astype(int)\n",
    "\n",
    "    df[feature_cols] = df[feature_cols].fillna(sentinal)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extend_features(df, sentinal, cols,\n",
    "        windows=[5, 10, 21, 63, 126, 252],\n",
    "        lags=[1, 2, 3, 5, 10, 21, 63, 126]\n",
    "    ):\n",
    "    df = df.copy()\n",
    "    feat_frames = []\n",
    "    new_cols = []\n",
    "\n",
    "    for col in [\n",
    "        c for c in cols \n",
    "        if not c.startswith(('MOM', 'VI'))]:\n",
    "            col_raw = df[col].astype(float)\n",
    "            col_shifted = col_raw.shift(1)\n",
    "\n",
    "            category_extensions = {}\n",
    "            \n",
    "            for window in windows:\n",
    "                roll = col_shifted.rolling(window, min_periods=5)\n",
    "                category_extensions[f'{col}_mean_{window}'] = roll.mean()\n",
    "                category_extensions[f'{col}_std_{window}'] = roll.std()\n",
    "                category_extensions[f'{col}_roc_{window}'] = col_shifted / col_shifted.shift(window) - 1\n",
    "                category_extensions[f'{col}_cum_{window}'] = roll.sum()\n",
    "\n",
    "            for lag in lags:\n",
    "                category_extensions[f'{col}_lag_{lag}'] = col_shifted.shift(lag-1)\n",
    "\n",
    "            ema_fast = col_shifted.ewm(span=10, adjust=False).mean()\n",
    "            ema_slow = col_shifted.ewm(span=50, adjust=False).mean()\n",
    "            category_extensions[f'{col}_ema_fast'] = ema_fast\n",
    "            category_extensions[f'{col}_ema_slow'] = ema_slow\n",
    "            category_extensions[f'{col}_ema_diff'] = ema_fast - ema_slow\n",
    "            category_extensions[f'{col}_ema_ratio'] = ema_fast / ema_slow - 1\n",
    "\n",
    "            feat_df = pd.DataFrame(category_extensions, index=df.index)\n",
    "            feat_frames.append(feat_df)\n",
    "            new_cols.extend(feat_df.columns.tolist())\n",
    "\n",
    "    feats = pd.concat(feat_frames, axis=1)\n",
    "\n",
    "    feats = feats.fillna(sentinal)\n",
    "\n",
    "    df_out = pd.concat([df, feats], axis=1)\n",
    "            \n",
    "    return df_out, new_cols\n",
    "\n",
    "def extend_all_features(df, sentinal, category_groups):\n",
    "    df_out = df.copy()\n",
    "    for category in [\n",
    "        c for c in category_groups\n",
    "        if not c.startswith((\"MOM\", \"VI\"))]:\n",
    "            print(category)\n",
    "            df_out, new_cols = extend_features(df_out, sentinal=sentinal, cols=category_groups[category])\n",
    "            print(f\"Added {len(new_cols)} new {category} features\")\n",
    "    return df_out\n",
    "\n",
    "def data_guards(df, sentinal, excluded):\n",
    "    df = df.copy()\n",
    "    excluded_cols = [c for c in excluded if c in df.columns]\n",
    "    df_features = df.drop(columns=excluded_cols)\n",
    "\n",
    "    X_all = df_features.select_dtypes(include=['number']).copy()\n",
    "    X_all = X_all.replace([np.inf, -np.inf], np.nan).fillna(sentinal)\n",
    "\n",
    "    const_cols = X_all.columns[X_all.nunique(dropna=False) <= 1]\n",
    "    X_all.drop(columns=const_cols, inplace=True)\n",
    "\n",
    "    variance_threshold = VarianceThreshold(threshold=1e-10)\n",
    "    X_reduced = pd.DataFrame(variance_threshold.fit_transform(X_all), index=X_all.index)\n",
    "    X_reduced.columns = X_all.columns[variance_threshold.get_support()]\n",
    "\n",
    "    df_out = pd.concat([df[excluded_cols], X_reduced], axis=1)\n",
    "    return df_out\n",
    "\n",
    "\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "df = add_date_col(df, start_date=START_DATE, periods=N_SAMPLES)\n",
    "df = sort_data_by_date_id(df, date_col=DATE_COL)\n",
    "df = create_target_log(df, target_col=TARGET_COL, arith_target_col=ARITH_TARGET_COL)\n",
    "category_groups = create_category_groups(df, excluded_col=EXCLUDED_COLS)\n",
    "df, category_groups = construct_momentum_cols(df, target_col=TARGET_COL, sentinal=SENTINAL_VALUE, category_groups=category_groups)\n",
    "df, category_groups = construct_volatility_indicators(df, target_col=TARGET_COL, sentinal=SENTINAL_VALUE, category_groups=category_groups)\n",
    "df = impute_independent_variables(df, sentinal=SENTINAL_VALUE, excluded=EXCLUDED_COLS)\n",
    "df = extend_all_features(df, sentinal=SENTINAL_VALUE, category_groups=category_groups)\n",
    "df = data_guards(df, sentinal=SENTINAL_VALUE, excluded=EXCLUDED_COLS)\n",
    "\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a7796184",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_INVESTMENT = 0.0\n",
    "MAX_INVESTMENT = 2.0\n",
    "TRADING_DAYS_PER_YR = 252\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "def adjusted_sharpe_from_df(solution: pd.DataFrame, submission: pd.DataFrame, row_id_col: str) -> float:\n",
    "    \"\"\"\n",
    "    solution: columns = [row_id_col, 'forward_returns', 'risk_free_rate']\n",
    "    submission: columns = [row_id_col, 'prediction'] where prediction ∈ [0,2]\n",
    "    \"\"\"\n",
    "    if not pd.api.types.is_numeric_dtype(submission['prediction']):\n",
    "        raise ParticipantVisibleError('Predictions must be numeric')\n",
    "    sol = solution.copy()\n",
    "    sub = submission.copy()\n",
    "\n",
    "    # align on row_id\n",
    "    sol = sol.merge(sub[[row_id_col, 'prediction']], on=row_id_col, how='inner')\n",
    "    sol.rename(columns={'prediction': 'position'}, inplace=True)\n",
    "\n",
    "    if sol['position'].max() > MAX_INVESTMENT:\n",
    "        raise ParticipantVisibleError(f'Position of {sol[\"position\"].max()} exceeds maximum of {MAX_INVESTMENT}')\n",
    "    if sol['position'].min() < MIN_INVESTMENT:\n",
    "        raise ParticipantVisibleError(f'Position of {sol[\"position\"].min()} below minimum of {MIN_INVESTMENT}')\n",
    "\n",
    "    # Strategy daily returns: convex combo of RF and market forward return\n",
    "    sol['strategy_returns'] = sol['risk_free_rate'] * (1 - sol['position']) + sol['position'] * sol['forward_returns']\n",
    "\n",
    "    # Strategy Sharpe (excess mean / std) annualized\n",
    "    strategy_excess = sol['strategy_returns'] - sol['risk_free_rate']\n",
    "    # geometric to arithmetic mean per-day\n",
    "    cum = (1 + strategy_excess).prod()\n",
    "    mean_excess_daily = cum ** (1 / len(sol)) - 1\n",
    "    std_daily = sol['strategy_returns'].std()\n",
    "    if std_daily == 0:\n",
    "        raise ParticipantVisibleError('Division by zero, strategy std is zero')\n",
    "\n",
    "    sharpe = mean_excess_daily / std_daily * np.sqrt(TRADING_DAYS_PER_YR)\n",
    "    strategy_vol_annual = float(std_daily * np.sqrt(TRADING_DAYS_PER_YR) * 100)\n",
    "\n",
    "    # Market stats (forward_returns vs risk_free_rate)\n",
    "    market_excess = sol['forward_returns'] - sol['risk_free_rate']\n",
    "    market_cum = (1 + market_excess).prod()\n",
    "    market_mean_excess_daily = market_cum ** (1 / len(sol)) - 1\n",
    "    market_std_daily = sol['forward_returns'].std()\n",
    "    if market_std_daily == 0:\n",
    "        raise ParticipantVisibleError('Division by zero, market std is zero')\n",
    "    market_vol_annual = float(market_std_daily * np.sqrt(TRADING_DAYS_PER_YR) * 100)\n",
    "\n",
    "    # Penalties (vol & return gap)\n",
    "    excess_vol = max(0, strategy_vol_annual / market_vol_annual - 1.2) if market_vol_annual > 0 else 0\n",
    "    vol_penalty = 1 + excess_vol\n",
    "\n",
    "    return_gap = max(0, (market_mean_excess_daily - mean_excess_daily) * 100 * TRADING_DAYS_PER_YR)\n",
    "    return_penalty = 1 + (return_gap ** 2) / 100\n",
    "\n",
    "    adjusted = sharpe / (vol_penalty * return_penalty)\n",
    "    return float(min(adjusted, 1_000_000.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f796c085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_to_position(\n",
    "    train_pred: np.ndarray,\n",
    "    test_pred: np.ndarray,\n",
    "    vol_factor_te: float,\n",
    "    rmse_factor_te: float,\n",
    "    regime_te: np.ndarray | None = None,\n",
    "    bad_regime_scale: float = 0.5):\n",
    "    \"\"\"\n",
    "    Standardize predictions on TRAIN, map to positions: pos = 1 + beta * z\n",
    "    Then clip to [0, 2].\n",
    "    beta controls aggressiveness; increase to take more risk.\n",
    "    \"\"\"\n",
    "    \n",
    "    beta_dynamic = 0.25 / (1 + 0.5*vol_factor_te + 0.5*rmse_factor_te)\n",
    "    \n",
    "    mu, sigma = np.mean(train_pred), np.std(train_pred)\n",
    "    if sigma == 0 or not np.isfinite(sigma):\n",
    "        pos = np.full_like(test_pred, 1.0, dtype=float)\n",
    "    else:\n",
    "        z = (test_pred - mu) / sigma\n",
    "        pos = 1.0 + beta_dynamic * z\n",
    "\n",
    "    if regime_te is not None:\n",
    "        # regime_te should be 0/1 with 1 = \"bad regime\" (e.g., high vol)\n",
    "        scaler = np.where(regime_te.astype(int) == 1, bad_regime_scale, 1.0)\n",
    "        pos = pos * scaler\n",
    "    \n",
    "    return np.clip(pos, MIN_INVESTMENT, MAX_INVESTMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "10df1f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Walk-Forward Validation with 24 folds (Test size: 252 days)\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Fold 1:\n",
      "  Train Size: 2,941\n",
      "  Test Size:  252\n",
      "  R-squared:  -0.010177\n",
      "  MSE:        0.000219\n",
      "  RMSE:       0.014782\n",
      "  MAE:        0.011464\n",
      "  Adjusted Sharpe: -1.1673720928009763\n",
      "  Best Iter:  59\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Fold 2:\n",
      "  Train Size: 3,193\n",
      "  Test Size:  252\n",
      "  R-squared:  -0.000474\n",
      "  MSE:        0.000188\n",
      "  RMSE:       0.013729\n",
      "  MAE:        0.010792\n",
      "  Adjusted Sharpe: 0.28671206388910697\n",
      "  Best Iter:  11\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Fold 3:\n",
      "  Train Size: 3,445\n",
      "  Test Size:  252\n",
      "  R-squared:  -0.011433\n",
      "  MSE:        0.000054\n",
      "  RMSE:       0.007364\n",
      "  MAE:        0.005812\n",
      "  Adjusted Sharpe: 0.8307669762265186\n",
      "  Best Iter:  276\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Fold 4:\n",
      "  Train Size: 3,697\n",
      "  Test Size:  252\n",
      "  R-squared:  0.005371\n",
      "  MSE:        0.000041\n",
      "  RMSE:       0.006368\n",
      "  MAE:        0.005059\n",
      "  Adjusted Sharpe: 0.8035062237120107\n",
      "  Best Iter:  12\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Fold 5:\n",
      "  Train Size: 3,949\n",
      "  Test Size:  252\n",
      "  R-squared:  0.006512\n",
      "  MSE:        0.000046\n",
      "  RMSE:       0.006817\n",
      "  MAE:        0.005142\n",
      "  Adjusted Sharpe: 0.4719275106966488\n",
      "  Best Iter:  55\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Fold 6:\n",
      "  Train Size: 4,201\n",
      "  Test Size:  252\n",
      "  R-squared:  0.010056\n",
      "  MSE:        0.000060\n",
      "  RMSE:       0.007739\n",
      "  MAE:        0.005289\n",
      "  Adjusted Sharpe: 1.1912114659342938\n",
      "  Best Iter:  78\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Fold 7:\n",
      "  Train Size: 4,453\n",
      "  Test Size:  252\n",
      "  R-squared:  -0.010375\n",
      "  MSE:        0.000154\n",
      "  RMSE:       0.012415\n",
      "  MAE:        0.009550\n",
      "  Adjusted Sharpe: -0.9094123560402597\n",
      "  Best Iter:  45\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 62\u001b[39m\n\u001b[32m     59\u001b[39m X_tr, y_tr = X_train_log.iloc[:-eval_tail], y_train_log.iloc[:-eval_tail]\n\u001b[32m     60\u001b[39m X_ev, y_ev = X_train_log.iloc[-eval_tail:], y_train_log.iloc[-eval_tail:]\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m \u001b[43mxgb_1_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_ev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_ev\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m y_hat_train_log = xgb_1_model.predict(X_train_log)\n\u001b[32m     69\u001b[39m y_hat_test_log =  xgb_1_model.predict(X_test_log)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\te25281\\Documents\\Hull Tactical Code\\Hull-Tactical---Market-Prediction-Kaggle-Competition-2025\\HullTactical\\Lib\\site-packages\\xgboost\\core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\te25281\\Documents\\Hull Tactical Code\\Hull-Tactical---Market-Prediction-Kaggle-Competition-2025\\HullTactical\\Lib\\site-packages\\xgboost\\sklearn.py:1365\u001b[39m, in \u001b[36mXGBModel.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1362\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1363\u001b[39m     obj = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1368\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[38;5;28mself\u001b[39m._set_evaluation_result(evals_result)\n\u001b[32m   1380\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\te25281\\Documents\\Hull Tactical Code\\Hull-Tactical---Market-Prediction-Kaggle-Competition-2025\\HullTactical\\Lib\\site-packages\\xgboost\\core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\te25281\\Documents\\Hull Tactical Code\\Hull-Tactical---Market-Prediction-Kaggle-Competition-2025\\HullTactical\\Lib\\site-packages\\xgboost\\training.py:199\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m \u001b[43mbst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\te25281\\Documents\\Hull Tactical Code\\Hull-Tactical---Market-Prediction-Kaggle-Competition-2025\\HullTactical\\Lib\\site-packages\\xgboost\\core.py:2434\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, dtrain, iteration, fobj)\u001b[39m\n\u001b[32m   2430\u001b[39m \u001b[38;5;28mself\u001b[39m._assign_dmatrix_features(dtrain)\n\u001b[32m   2432\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2433\u001b[39m     _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2434\u001b[39m         \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2435\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\n\u001b[32m   2436\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2437\u001b[39m     )\n\u001b[32m   2438\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2439\u001b[39m     pred = \u001b[38;5;28mself\u001b[39m.predict(dtrain, output_margin=\u001b[38;5;28;01mTrue\u001b[39;00m, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "N_SPLITS = 24\n",
    "TEST_SIZE = 252\n",
    "EARLY_STOPPING_ROUNDS = 100\n",
    "gap = 1\n",
    "\n",
    "ROW_ID_COL = \"row_id\"\n",
    "df_eval = df.copy()\n",
    "df_eval[ROW_ID_COL] = df_eval.index\n",
    "\n",
    "y = df['log_market_forward_excess_returns']\n",
    "X = df.drop(EXCLUDED_COLS, axis=1).copy()\n",
    "\n",
    "tscv = TimeSeriesSplit(\n",
    "    n_splits = N_SPLITS,\n",
    "    test_size = TEST_SIZE\n",
    ")\n",
    "\n",
    "xgb_1_model = xgb.XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    tree_method=\"hist\",\n",
    "    random_state=42,\n",
    "    n_estimators=5000,\n",
    "    learning_rate=0.08,\n",
    "    max_depth=5,\n",
    "    min_child_weight=2,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=5,\n",
    "    reg_alpha=1.0,\n",
    "    gamma=0.0,\n",
    "    eval_metric=\"rmse\",\n",
    "    early_stopping_rounds=EARLY_STOPPING_ROUNDS\n",
    ")\n",
    "\n",
    "xgb_r2_scores = []\n",
    "xgb_mse_scores = []\n",
    "xgb_rmse_scores = []\n",
    "xgb_mae_scores = []\n",
    "xgb_adj_sharpes = []\n",
    "predictions = []\n",
    "fold_metrics = []\n",
    "\n",
    "print(f\"Starting Walk-Forward Validation with {N_SPLITS} folds (Test size: {TEST_SIZE} days)\")\n",
    "print(\"\\n\" + (\"-\" * 50) + \"\\n\")\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "\n",
    "    if gap > 0:\n",
    "        max_train_index = test_index[0] - gap\n",
    "        train_index = train_index[train_index < max_train_index]\n",
    "\n",
    "    \n",
    "    X_train_log, X_test_log = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_log, y_test_log = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Early stopping eval set\n",
    "    eval_tail = min(len(X_train_log)//5, 252)  # up to last ~252 days or 20% of train\n",
    "    eval_tail = max(eval_tail, 50)         # ensure some minimum\n",
    "    X_tr, y_tr = X_train_log.iloc[:-eval_tail], y_train_log.iloc[:-eval_tail]\n",
    "    X_ev, y_ev = X_train_log.iloc[-eval_tail:], y_train_log.iloc[-eval_tail:]\n",
    "\n",
    "    xgb_1_model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_ev, y_ev)],\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    y_hat_train_log = xgb_1_model.predict(X_train_log)\n",
    "    y_hat_test_log =  xgb_1_model.predict(X_test_log)\n",
    "    \n",
    "    fold_r2 = r2_score(y_test_log, y_hat_test_log)\n",
    "    fold_mse = mean_squared_error(y_test_log, y_hat_test_log)\n",
    "    fold_rmse = root_mean_squared_error(y_test_log, y_hat_test_log)\n",
    "    fold_mae = mean_absolute_error(y_test_log, y_hat_test_log)\n",
    "    \n",
    "    xgb_r2_scores.append(fold_r2)\n",
    "    xgb_mse_scores.append(fold_mse)\n",
    "    xgb_rmse_scores.append(fold_rmse)\n",
    "    xgb_mae_scores.append(fold_mae)\n",
    "\n",
    "    vol21 = df_eval.loc[X_test_log.index, \"MOM_std_21\"].astype(float)\n",
    "    vol_factor_te = (vol21 / df_eval[\"MOM_std_21\"].median())\n",
    "    \n",
    "    train_resid = (y_train_log - y_hat_train_log)\n",
    "    sq = pd.Series(train_resid**2, index=X_train_log.index)\n",
    "    \n",
    "    # last-126d RMSE at the fold boundary (use min_periods to avoid NaNs)\n",
    "    rmse_tail = np.sqrt(sq.rolling(126, min_periods=50).mean().iloc[-1])\n",
    "    # a baseline (median) RMSE over the train period\n",
    "    rmse_base = float(np.sqrt(sq.rolling(252, min_periods=50).mean()).median())\n",
    "    if not np.isfinite(rmse_base) or rmse_base == 0:\n",
    "        rmse_base = max(rmse_tail, 1e-6)\n",
    "    \n",
    "    # same scalar applied to all rows in the TEST slice (no look-ahead)\n",
    "    rmse_factor_te = np.full(len(X_test_log), rmse_tail / rmse_base)\n",
    "    \n",
    "    reg_te = df_eval.loc[X_test_log.index, \"VI_regime_highvol\"].to_numpy()\n",
    "    pos_te = signal_to_position(\n",
    "        y_hat_train_log,\n",
    "        y_hat_test_log,\n",
    "        vol_factor_te=vol_factor_te,\n",
    "        rmse_factor_te=rmse_factor_te,\n",
    "        regime_te=reg_te,\n",
    "        bad_regime_scale=0.5)\n",
    "    \n",
    "    # pos_te = signal_to_position_percentile(y_hat_tr, y_hat_te, lo=5, hi=95)\n",
    "\n",
    "    solution_fold = df_eval.loc[X_test_log.index, [ROW_ID_COL, \"forward_returns\", \"risk_free_rate\"]].copy()\n",
    "    submission_fold = pd.DataFrame({ROW_ID_COL: df_eval.loc[X_test_log.index, ROW_ID_COL].values,\"prediction\": pos_te.astype(float)})\n",
    "\n",
    "    # Competition metric\n",
    "    fold_adjusted_sharpe = adjusted_sharpe_from_df(solution_fold, submission_fold, row_id_col=ROW_ID_COL)\n",
    "    xgb_adj_sharpes.append(fold_adjusted_sharpe)\n",
    "\n",
    "    predictions.append(pd.DataFrame({\n",
    "        \"date\": df_eval.loc[X_test_log.index, \"date\"].values,\n",
    "        \"y_log\": y_test_log.values,\n",
    "        \"y_arith\": np.expm1(y_test_log.values),\n",
    "        \"y_pred_log\": y_hat_test_log,\n",
    "        \"y_pred_arith\": np.expm1(y_hat_test_log),\n",
    "        \"position\": pos_te.astype(float),\n",
    "        \"fold\": fold + 1,\n",
    "    }))\n",
    "    \n",
    "    print(f\"Fold {fold + 1}:\")\n",
    "    print(f\"  Train Size: {len(X_train_log):,}\")\n",
    "    print(f\"  Test Size:  {len(X_test_log):,}\")\n",
    "    print(f\"  R-squared:  {fold_r2:.6f}\")\n",
    "    print(f\"  MSE:        {fold_mse:.6f}\")\n",
    "    print(f\"  RMSE:       {fold_rmse:.6f}\")\n",
    "    print(f\"  MAE:        {fold_mae:.6f}\")\n",
    "    print(f\"  Adjusted Sharpe: {fold_adjusted_sharpe}\")\n",
    "    print(f\"  Best Iter:  {getattr(xgb_1_model, 'best_iteration', None)}\")\n",
    "    print(\"\\n\" + (\"-\" * 50) + \"\\n\")\n",
    "\n",
    "    fold_metrics.append({\n",
    "        \"fold\": fold + 1,\n",
    "        \"train_start\": df_eval.loc[train_index[0], \"date\"],\n",
    "        \"train_end\":   df_eval.loc[train_index[-1], \"date\"],\n",
    "        \"test_start\":  df_eval.loc[test_index[0], \"date\"],\n",
    "        \"test_end\":    df_eval.loc[test_index[-1], \"date\"],\n",
    "        \"train_size\": len(X_train_log),\n",
    "        \"test_size\": len(X_test_log),\n",
    "        \"best_iteration\": getattr(xgb_1_model, \"best_iteration\", None),\n",
    "        \"R-squared\": float(fold_r2),\n",
    "        \"mse\": float(fold_mse),\n",
    "        \"rmse\": float(fold_rmse),\n",
    "        \"mae\": float(fold_mae),\n",
    "        \"Adjusted Sharpe\": float(fold_adjusted_sharpe)\n",
    "    })\n",
    "\n",
    "\n",
    "print(\"\\n--- Final Walk-Forward Results ---\")\n",
    "print(f\"Average R-squared: {np.mean(xgb_r2_scores):.6f} ± {np.std(xgb_r2_scores):.6f}\")\n",
    "print(f\"Average MSE:       {np.mean(xgb_mse_scores):.6f} ± {np.std(xgb_mse_scores):.6f}\")\n",
    "print(f\"Average RMSE:      {np.mean(xgb_rmse_scores):.6f} ± {np.std(xgb_rmse_scores):.6f}\")\n",
    "print(f\"Average MAE:       {np.mean(xgb_mae_scores):.6f} ± {np.std(xgb_mae_scores):.6f}\")\n",
    "print(f\"AdjSharpe:         {np.mean(xgb_adj_sharpes):.6f} ± {np.std(xgb_adj_sharpes):.6f}\")\n",
    "print(f\"MaxSharpe:         {np.max(xgb_adj_sharpes):.6f}\")\n",
    "\n",
    "oof_df = pd.concat(predictions, ignore_index=True).sort_values(\"date\").reset_index(drop=True)\n",
    "fold_df = pd.DataFrame(fold_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b62767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cumulative_returns(oof_df: pd.DataFrame, threshold=0.0):\n",
    "    # simple long/flat strategy: go long if prediction > threshold\n",
    "    pos = oof_df[\"position\"].astype(float).clip(0, 2)\n",
    "    ret = oof_df[\"y_arith\"].astype(float)\n",
    "    strat_ret = pos * ret\n",
    "    cum_strat = (1 + strat_ret).cumprod()\n",
    "    cum_mkt   = (1 + ret).cumprod()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    ax.plot(oof_df[\"date\"], cum_mkt, label=\"Cumulative Actual\")\n",
    "    ax.plot(oof_df[\"date\"], cum_strat, label=f\"Cumulative Strategy (position)\")\n",
    "    ax.set_title(\"Cumulative Returns (OOF)\")\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Growth of 1 unit\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_predictions_over_time(oof_df, title=\"Actual vs Predicted (OOF, log)\"):\n",
    "    df = oof_df.sort_values(\"date\")\n",
    "    fig, ax = plt.subplots(figsize=(12,5))\n",
    "    ax.plot(df[\"date\"], df[\"y_log\"],      label=\"Actual (log)\")\n",
    "    ax.plot(df[\"date\"], df[\"y_pred_log\"], label=\"Predicted (log)\")\n",
    "    ax.set_title(title); ax.set_xlabel(\"Date\"); ax.set_ylabel(\"log(1+r)\")\n",
    "    ax.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "def plot_predictions_over_time_arith(oof_df, title=\"Actual vs Predicted (OOF, arith)\"):\n",
    "    df = oof_df.sort_values(\"date\")\n",
    "    fig, ax = plt.subplots(figsize=(12,5))\n",
    "    ax.plot(df[\"date\"], df[\"y_arith\"],       label=\"Actual (arith)\")\n",
    "    ax.plot(df[\"date\"], df[\"y_pred_arith\"],  label=\"Predicted (arith)\")\n",
    "    ax.set_title(title); ax.set_xlabel(\"Date\"); ax.set_ylabel(\"Return\")\n",
    "    ax.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "def plot_fold_metrics(fold_df: pd.DataFrame):\n",
    "    # bar/line of per-fold RMSE over chronological test windows\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    ax.plot(fold_df[\"fold\"], fold_df[\"rmse\"], marker=\"o\")\n",
    "    ax.set_title(\"Per-fold RMSE (walk-forward)\")\n",
    "    ax.set_xlabel(\"Fold (chronological)\")\n",
    "    ax.set_ylabel(\"RMSE\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_rolling_metrics(oof_df: pd.DataFrame, window=126):\n",
    "    # rolling RMSE and rolling correlation to see stability\n",
    "    df = oof_df.copy()\n",
    "    df[\"squared_err\"] = (df[\"y_arith\"] - df[\"y_pred_arith\"])**2\n",
    "    df[\"roll_rmse\"] = (df[\"squared_err\"].rolling(window).mean())**0.5\n",
    "    df[\"roll_corr\"] = df[\"y_arith\"].rolling(window).corr(df[\"y_pred_arith\"])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    ax.plot(df[\"date\"], df[\"roll_rmse\"])\n",
    "    ax.set_title(f\"Rolling RMSE (window={window})\")\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"RMSE\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    ax.plot(df[\"date\"], df[\"roll_corr\"])\n",
    "    ax.set_title(f\"Rolling Correlation (window={window})\")\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Correlation\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_scatter_pred_vs_actual(oof_df: pd.DataFrame):\n",
    "    X = oof_df[\"y_pred_arith\"].values.reshape(-1, 1)\n",
    "    y = oof_df[\"y_arith\"].values\n",
    "\n",
    "    lr = LinearRegression().fit(X, y)\n",
    "    x_grid = np.linspace(X.min(), X.max(), 200).reshape(-1, 1)\n",
    "    y_fit = lr.predict(x_grid)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.scatter(oof_df[\"y_pred_arith\"], oof_df[\"y_arith\"], alpha=0.4)\n",
    "    ax.plot(x_grid, y_fit, linewidth=2, label=f\"Fit: y={lr.coef_[0]:.2f}*ŷ + {lr.intercept_:.4f}\")\n",
    "    ax.axline((0,0), slope=1, linestyle=\"--\", label=\"45° line (perfect)\")\n",
    "    ax.set_xlabel(\"Predicted return\")\n",
    "    ax.set_ylabel(\"Actual return\")\n",
    "    ax.set_title(\"Predicted vs Actual (OOF)\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_smoothed(oof_df, win=21):\n",
    "    df = oof_df.sort_values(\"date\").copy()\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(df[\"date\"], df[\"y_log\"].rolling(win).mean(),      label=f\"Actual {win}d MA\")\n",
    "    plt.plot(df[\"date\"], df[\"y_pred_log\"].rolling(win).mean(), label=f\"Pred {win}d MA\")\n",
    "    plt.title(\"Actual vs Predicted (smoothed)\"); plt.xlabel(\"Date\")\n",
    "    plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "def plot_directional_accuracy(oof_df, window=252):\n",
    "    df = oof_df.copy()\n",
    "    df[\"correct\"] = (np.sign(df[\"y_arith\"]) == np.sign(df[\"y_pred_arith\"])).astype(int)\n",
    "    df[\"rolling_acc\"] = df[\"correct\"].rolling(window).mean()\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(df[\"date\"], df[\"rolling_acc\"])\n",
    "    plt.title(f\"Rolling Directional Accuracy ({window}-day window)\")\n",
    "    plt.axhline(0.5, color='red', linestyle='--')\n",
    "    plt.ylabel(\"Accuracy\"); plt.xlabel(\"Date\")\n",
    "    plt.show()\n",
    "\n",
    "plot_predictions_over_time(oof_df)\n",
    "plot_predictions_over_time_arith(oof_df)\n",
    "plot_fold_metrics(fold_df)\n",
    "plot_rolling_metrics(oof_df)\n",
    "plot_scatter_pred_vs_actual(oof_df)\n",
    "plot_cumulative_returns(oof_df)\n",
    "plot_smoothed(oof_df)\n",
    "plot_directional_accuracy(oof_df)\n",
    "xgb.plot_importance(xgb_1_model, max_num_features=20)\n",
    "\n",
    "for i, fold in enumerate(np.unique(oof_df[\"fold\"])):\n",
    "    fold_df = oof_df[oof_df[\"fold\"] == fold]\n",
    "    ic = np.corrcoef(fold_df[\"y_arith\"], fold_df[\"y_pred_arith\"])[0,1]\n",
    "    print(f\"Fold {i+1}: IC={ic:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd32b07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRADING_DAYS = 252\n",
    "\n",
    "def perf_metrics(oof_df, df_eval):\n",
    "    # Align with true market forward return and rf (arithmetic, not excess)\n",
    "    base = df_eval[[\"date\", \"forward_returns\", \"risk_free_rate\"]].copy()\n",
    "    z = oof_df[[\"date\",\"position\"]].merge(base, on=\"date\", how=\"inner\").dropna()\n",
    "\n",
    "    pos = z[\"position\"].astype(float).clip(0, 2)\n",
    "    mkt = z[\"forward_returns\"].astype(float)\n",
    "    rf  = z[\"risk_free_rate\"].astype(float) if \"risk_free_rate\" in z else 0.0\n",
    "\n",
    "    # Strategy daily return (same formula you used for Adjusted Sharpe)\n",
    "    strat = rf*(1 - pos) + pos*mkt\n",
    "    excess = strat - rf\n",
    "\n",
    "    # --- CAGR / total return ---\n",
    "    n_years = (z[\"date\"].iloc[-1] - z[\"date\"].iloc[0]).days / 365.25\n",
    "    total_return = (1.0 + strat).prod() - 1.0\n",
    "    cagr = (1.0 + total_return)**(1.0 / n_years) - 1.0\n",
    "\n",
    "    # --- Sharpe (for reference) ---\n",
    "    mu_e = excess.mean()\n",
    "    sd   = strat.std()\n",
    "    sharpe = (mu_e / sd) * np.sqrt(TRADING_DAYS) if sd > 0 else np.nan\n",
    "\n",
    "    # --- Sortino ---\n",
    "    downside = excess[excess < 0]\n",
    "    dd = downside.std()  # sample std; use ddof=0 if you prefer population\n",
    "    sortino = (mu_e * TRADING_DAYS) / (dd * np.sqrt(TRADING_DAYS)) if dd > 0 else np.nan\n",
    "\n",
    "    # --- Extras: drawdown & turnover (useful context) ---\n",
    "    equity = (1.0 + strat).cumprod()\n",
    "    roll_max = equity.cummax()\n",
    "    drawdown = equity / roll_max - 1.0\n",
    "    max_dd = drawdown.min()\n",
    "\n",
    "    turnover = pos.diff().abs().fillna(0).sum() / len(pos)  # avg daily turnover in units\n",
    "\n",
    "    return {\n",
    "        \"Total Return\": total_return,\n",
    "        \"CAGR\": cagr,\n",
    "        \"Sharpe\": sharpe,\n",
    "        \"Sortino\": sortino,\n",
    "        \"Max Drawdown\": max_dd,\n",
    "        \"Avg Daily Turnover\": turnover,\n",
    "        \"Obs\": len(z)\n",
    "    }\n",
    "\n",
    "metrics = perf_metrics(oof_df, df_eval)\n",
    "for k,v in metrics.items():\n",
    "    print(f\"{k:>18}: {v:.6f}\" if isinstance(v, (int,float,np.floating)) else f\"{k:>18}: {v}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HullTactical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
